{"nbformat_minor": 2, "cells": [{"source": "# AutoML 101: auto-ml-classification\n\nIn this example we showcase how you can use the AutoML Classifier for a simple classification problem.\n\nIn this notebook you would see\n1. Creating or reusing an existing Project and Workspace\n2. Instantiating AutoML Classifier\n3. Training the Model using local compute\n4. Exploring the results\n5. Testing the fitted model", "cell_type": "markdown", "metadata": {}}, {"execution_count": null, "cell_type": "code", "metadata": {"collapsed": true}, "outputs": [], "source": "import azureml.core\nimport pandas as pd\nfrom azureml.core.authentication import ServicePrincipalAuthentication\nfrom azureml.core.workspace import Workspace\nfrom azureml.core.experiment import Experiment\nfrom azureml.train.automl import AutoMLConfig\nfrom azureml.train.automl.run import AutoMLRun\nimport logging"}, {"execution_count": null, "cell_type": "code", "metadata": {"collapsed": false}, "outputs": [], "source": "subscription_id = \"<Azure Subscription ID>\"\nresource_group = \"<Azure Resource Group>\"\nworkspace_name = \"AMLSample\"\nworkspace_region = \"eastus2\"\n\ntenant_id = \"<Azure Tenant ID>\"\napp_id = \"<Azure AD Application ID>\"\napp_key = \"<Azure AD Application Key>\"\n\nauth_sp = ServicePrincipalAuthentication(tenant_id = tenant_id,\n                                         username = app_id,\n                                         password = app_key)"}, {"execution_count": null, "cell_type": "code", "metadata": {"collapsed": false}, "outputs": [], "source": "# import the Workspace class\nfrom azureml.core import Workspace\n\nws = Workspace.create(name = workspace_name,\n                      auth = auth_sp,\n                      subscription_id = subscription_id,\n                      resource_group = resource_group, \n                      create_resource_group = True,\n                      location = workspace_region,\n                      exist_ok = True)\n\nws.get_details()"}, {"execution_count": null, "cell_type": "code", "metadata": {"collapsed": false}, "outputs": [], "source": "from azureml.core.experiment import Experiment\n\n# choose a name for experiment\nexperiment_name = 'automl-classification'\n# project folder\nproject_folder = '/home/.azureml'\n\nexperiment=Experiment(ws, experiment_name)\n\noutput = {}\noutput['SDK version'] = azureml.core.VERSION\noutput['Subscription ID'] = ws.subscription_id\noutput['Workspace Name'] = ws.name\noutput['Resource Group'] = ws.resource_group\noutput['Location'] = ws.location\noutput['Project Directory'] = project_folder\noutput['Experiment Name'] = experiment.name\npd.set_option('display.max_colwidth', -1)\npd.DataFrame(data = output, index = ['']).T"}, {"source": "## Diagnostics\n\nOpt-in diagnostics collection for better experience, quality, and security of future releases", "cell_type": "markdown", "metadata": {}}, {"execution_count": null, "cell_type": "code", "metadata": {"collapsed": true}, "outputs": [], "source": "from azureml.telemetry import set_diagnostics_collection\nset_diagnostics_collection(send_diagnostics=True)"}, {"source": "## Load BlobStore Dataset", "cell_type": "markdown", "metadata": {}}, {"execution_count": null, "cell_type": "code", "metadata": {"collapsed": false}, "outputs": [], "source": "import azureml.dataprep as dprep\n\ndataflow = dprep.read_csv(path='https://commonartifacts.blob.core.windows.net/automl/UCI_Adult_train.csv')\nX_train = dataflow.drop_columns('label(IsOver50K)').skip(100)\ny_train = dataflow.keep_columns('label(IsOver50K)').skip(100)"}, {"source": "## Instantiate Auto ML Classifier\n\nInstantiate a AutoML Object This creates an Experiment in Azure ML. You can reuse this objects to trigger multiple runs. Each run will be part of the same experiment.\n\n|Property|Description|\n|-|-|\n|**primary_metric**|This is the metric that you want to optimize.<br> Auto ML Classifier supports the following primary metrics <br><i>AUC_macro</i><br><i>AUC_weighted</i><br><i>accuracy</i><br><i>weighted_accuracy</i><br><i>norm_macro_recall</i><br><i>balanced_accuracy</i><br><i>average_precision_score_weighted</i>|\n|**max_time_sec**|Time limit in seconds for each iterations|\n|**iterations**|Number of iterations. In each iteration Auto ML Classifier trains the data with a specific pipeline|\n|**n_cross_validations**|Number of cross validation splits|\n|**verbosity**|Verbosity level for AutoML log file|\n|**X**|The training features to use when fitting pipelines during AutoML experiment|\n|**y**|Training labels to use when fitting pipelines during AutoML experiment|\n|**preprocess**|Flag whether AutoML should preprocess your data for you such as handling missing data, text data and other common feature extraction. Note: If input data is Sparse you cannot use preprocess as True|\n|**concurrent_iterations**|Maximum number of iterations that would be executed in parallel. This should be less than the number of cores on the AzureML compute|\n|**spark_context**|Spark context|\n|**path**|Path to the AzureML project folder|", "cell_type": "markdown", "metadata": {}}, {"execution_count": null, "cell_type": "code", "metadata": {"collapsed": true}, "outputs": [], "source": "from azureml.train.automl import AutoMLConfig\n\nautoml_config = AutoMLConfig(task = 'classification',\n                             primary_metric = 'accuracy',\n                             max_time_sec = 3600,\n                             iterations = 5,\n                             n_cross_validations = 2,\n                             verbosity = logging.INFO,\n                             X = X_train, \n                             y = y_train,\n                             preprocess = True,\n                             concurrent_iterations=5,\n                             spark_context = sc,\n                             path=project_folder,\n                             enable_cache=False \n                             )"}, {"source": "## Training the Model\n\nYou can call the fit method on the AutoML instance and pass the run configuration. Depending on the data and number of iterations this can run for while. Once the run is complete, iteration results will be printed to console.\n\n*fit* method on Auto ML Classifier triggers the training of the model. It can be called with the following parameters\n\n|**Parameter**|**Description**|\n|-|-|\n|**automl_config**|AutoML config instantiated in the previous step|\n|**show_output**| True/False to turn on/off console output|", "cell_type": "markdown", "metadata": {}}, {"execution_count": null, "cell_type": "code", "metadata": {"collapsed": false}, "outputs": [], "source": "from azureml.train.automl.run import AutoMLRun\n\nexpt_run = experiment.submit(automl_config, show_output=True)"}, {"source": "## Exploring the results", "cell_type": "markdown", "metadata": {}}, {"source": "#### Widget for monitoring runs\n\nThe widget will sit on \"loading\" until the first iteration completed, then you will see an auto-updating graph and table show up. It refreshed once per minute, so you should see the graph update as child runs complete.\n\nNOTE: The widget displays a link at the bottom. This links to a web-ui to explore the individual run details.", "cell_type": "markdown", "metadata": {}}, {"execution_count": null, "cell_type": "code", "metadata": {"collapsed": false}, "outputs": [], "source": "print(expt_run.get_portal_url())"}, {"source": "#### Retrieve All Child Runs\nYou can also use sdk methods to fetch all the child runs and see individual metrics that we log.", "cell_type": "markdown", "metadata": {}}, {"execution_count": null, "cell_type": "code", "metadata": {"collapsed": false}, "outputs": [], "source": "children = list(expt_run.get_children())\nmetricslist = {}\nfor run in children:\n    properties = run.get_properties()\n    metrics = {k: v for k, v in run.get_metrics().items() if isinstance(v, float)}    \n    metricslist[int(properties['iteration'])] = metrics\n    \nimport pandas as pd\n\nrundata = pd.DataFrame(metricslist).sort_index(1)\nrundata"}, {"source": "### Retrieve the Best Model\n\nBelow we select the best pipeline from our iterations. The *get_output* method on automl_classifier returns the best run and the fitted model for the last *fit* invocation. There are overloads on *get_output* that allow you to retrieve the best run and fitted model for *any* logged metric or a particular *iteration*.", "cell_type": "markdown", "metadata": {}}, {"execution_count": null, "cell_type": "code", "metadata": {"collapsed": false}, "outputs": [], "source": "best_run, fitted_model = expt_run.get_output()\nprint(best_run)\nprint(fitted_model)"}, {"source": "#### Best Model based on any other metric\nGive me the run and the model that has the smallest `log_loss`:", "cell_type": "markdown", "metadata": {}}, {"execution_count": null, "cell_type": "code", "metadata": {"collapsed": false}, "outputs": [], "source": "lookup_metric = \"log_loss\"\nbest_run, fitted_model = expt_run.get_output(metric = lookup_metric)\nprint(best_run)\nprint(fitted_model)"}, {"execution_count": null, "cell_type": "code", "metadata": {"collapsed": false}, "outputs": [], "source": "iteration = 3\nbest_run, fitted_model = expt_run.get_output(iteration = iteration)\nprint(best_run)\nprint(fitted_model)"}, {"source": "### Test the Best Fitted Model\n\n#### Load Test Data", "cell_type": "markdown", "metadata": {}}, {"execution_count": null, "cell_type": "code", "metadata": {"collapsed": true}, "outputs": [], "source": "from sklearn import datasets\ndigits = datasets.load_digits()\nX_test = digits.data[:10, :]\ny_test = digits.target[:10]\nimages = digits.images[:10]"}, {"source": "#### Testing our best pipeline\nWe will try to predict 2 digits and see how our model works.", "cell_type": "markdown", "metadata": {}}, {"execution_count": null, "cell_type": "code", "metadata": {"collapsed": true}, "outputs": [], "source": "#Randomly select digits and test\nimport random\nimport numpy as np\n\nfor index in np.random.choice(len(y_test), 2):\n    predicted = fitted_model.predict(X_test[index:index + 1])[0]\n    label = y_test[index]\n    compare = \"Index:%d Label value = %s  Predicted value = %s \" % (index,label,predicted)\n    print(compare)"}], "metadata": {"kernelspec": {"display_name": "PySpark3", "name": "pyspark3kernel", "language": ""}, "language_info": {"mimetype": "text/x-python", "pygments_lexer": "python3", "name": "pyspark3", "codemirror_mode": {"version": 3, "name": "python"}}}, "nbformat": 4}