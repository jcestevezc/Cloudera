{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Architecture Intro.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyM6XuDIQC65P9+g8nOOKqJw",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jcestevezc/Cloudera/blob/master/Architecture%20Intro/Architecture_Intro.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CuhaZgZB0Nz9",
        "colab_type": "text"
      },
      "source": [
        "# Big Data Architecture"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6ZLf8MFYypYF",
        "colab_type": "text"
      },
      "source": [
        "The Big Data Architecture helps in designing Data Pipelines with the various requirements of either the ***Batch Processing System or Stream Processing System***.\n",
        "\n",
        "The architecture consists of 6 layers which ensure a secure flow of data.\n",
        "\n",
        "![Architecture](https://github.com/jcestevezc/Cloudera/blob/master/Architecture%20Intro/big-data-framework.png?raw=true)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZubVWr900Ttc",
        "colab_type": "text"
      },
      "source": [
        "## Data Ingestion Layer /  Data Collector Layer\n",
        "\n",
        "* Involves connecting to various data sources, extracting the data, and detecting the changed data. \n",
        "\n",
        "* Ingestion is the process of bringing data into the Data Processing system. \n",
        "\n",
        "* We can also say that Data Ingestion means taking data coming from multiple sources and putting it somewhere it can be accessed. \n",
        "\n",
        "* Data can be streamed in real-time (as soon as data arrives it is ingested immediately) or ingested in batches (data items are ingested in some chunks at a periodic interval of time). \n",
        "\n",
        "* Effective Data Ingestion process begins by prioritizing data sources, validating individual files and routing data items to the correct destination.\n",
        "\n",
        "* Detection and capture of changed data – This task is difficult, not only because of the semi-structured or unstructured nature of data but also due to the low latency needed by individual business scenarios that require this determination.\n",
        "\n",
        "### Tools \n",
        "\n",
        "* Apache flume: is a distributed, reliable, and available service for efficiently collecting, aggregating, and moving large amounts of log data.\n",
        "It has a straightforward and flexible architecture based on streaming data flows. It is robust and faults tolerant with tunable reliability mechanisms and many failovers and recovery mechanisms.\n",
        "It uses a simple, extensible data model that allows for an online analytic application.\n",
        "* Apache Nifi: provides an easy to use, the powerful, and reliable system to process and distribute data. Apache NiFi **supports robust and scalable directed graphs** of data routing, transformation, and system mediation logic. \n",
        "\n",
        "* Sqoop: it efficiently transfers bulk data between Apache Hadoop and structured datastores such as relational databases. Apache Sqoop can also be used to extract data from Hadoop and export it into external structured data stores. Apache Sqoop works with relational databases such as Teradata, Netezza, Oracle, MySQL, Postgres, and HSQLDB.\n",
        "* Apache Kafka: it is used for building real-time data pipelines and streaming apps. It can process streams of data in real-time and store streams of data safely in a distributed replicated cluster.\n",
        "Kafka works in combination with Apache Storm, Apache HBase and Apache Spark for real-time analysis and rendering of streaming data.\n",
        "Building Real-Time streaming Data Pipelines that reliably get data between systems or applications."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ai8QYCLxN08o",
        "colab_type": "text"
      },
      "source": [
        "## Data Processing Layer\n",
        "\n",
        "The focus is to specialize in the data pipeline processing system, or we can say the data we have collected in the previous layer is to be processed in this layer.\n",
        "\n",
        "### Tools\n",
        "\n",
        "* Apache Storm: it is a system for processing streaming data in real time. It adds reliable real-time data processing capabilities to Enterprise Hadoop. Storm on YARN is powerful for scenarios requiring real-time analytics, machine learning and continuous monitoring of operations.\n",
        "\n",
        "* Apache Spark: is a fast, in-memory data processing engine with elegant and expressive development APIs to allow data workers to efficiently execute streaming, machine learning or SQL workloads that require fast iterative access to data sets. With Spark running on Apache Hadoop YARN, developers everywhere can now create applications to exploit Spark’s power, derive insights, and enrich their data science workloads within a single, shared data set in Hadoop.\n",
        "\n",
        "* Apache Flink: is an open-source framework for distributed stream processing that Provides results that are accurate, even in the case of out-of-order or late-arriving data. It’s streaming data flow execution engine, APIs and domain-specific libraries for Batch, Streaming, Machine Learning, and Graph Processing."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EOWTUgtpN9Ms",
        "colab_type": "text"
      },
      "source": [
        "## Data Storage Layer\n",
        "\n",
        "This layer focuses on “where to store such large and multiformat data efficiently.”\n",
        "\n",
        "### Tools\n",
        "\n",
        "* HDFS (Hadoop Distributed File System): HDFS is a Java-based file system that provides scalable and reliable data storage, and it was designed to span large clusters of commodity servers. HDFS holds a huge amount of data and provides easier access. To store such massive data, the files are stored on multiple machines. These files are stored redundantly to rescue the system from possible data losses in case of failure. HDFS also makes applications available for parallel processing. HDFS is built to support applications with large data sets, including individual files that reach into the terabytes. It uses a master/slave architecture, with each cluster consisting of a single NameNode that manages file system operations and supporting DataNodes that manage data storage on individual compute nodes. When HDFS takes in data, it breaks the information down into separate pieces and distributes them to different nodes in a cluster, allowing for parallel processing. The file system also copies each piece of data multiple times and distributes the copies to individual nodes, placing at least one copy on a different server rack.\n",
        "\n",
        "* Gluster: As we know good storage solution must provide elasticity in both storage and performance without affecting active operations. Scale-out storage systems based on GlusterFS are suitable for unstructured data such as documents, images, audio and video files, and log files. GlusterFS is a scalable network filesystem. Using this, we can create large, distributed storage solutions for media streaming, data analysis, and other data- and bandwidth-intensive tasks.\n",
        "\n",
        "* Amazon Simple Storage Service (Amazon S3): is object storage with a simple web service interface to store and retrieve any amount of data from anywhere on the internet. Customers use S3 as primary storage for cloud-native applications; as a bulk repository, or “data lake,” for analytics; as a target for backup & recovery and disaster recovery; and with serverless computing.\n",
        "It’s simple to move large volumes of data into or out of S3 with Amazon’s cloud data migration options. Once data is stored on Amazon S3, it can be automatically tiered into lower cost, longer-term cloud storage classes like S3 Standard – Infrequent Access and Amazon Glacier for archiving."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HDhzZwykODZM",
        "colab_type": "text"
      },
      "source": [
        "## Data Query Layer\n",
        "\n",
        "This is the layer where active analytic processing takes place. This is a field where interactive queries are necessaries, and it’s a zone traditionally dominated by SQL expert developers.\n",
        "\n",
        "### Tools\n",
        "\n",
        "* Apache Hive: is data warehouse infrastructure built on top of Apache Hadoop for providing data summarization, ad-hoc query, and analysis of large datasets.\n",
        "Data analysts use Hive to query, summarize, explore and analyze that data, then turn it into actionable business insight. It provides a mechanism to project structure onto the data in Hadoop and to query that data using a SQL – like a language called HiveQL (HQL).\n",
        "\n",
        "* Apache Spark SQL: includes a cost-based optimizer, columnar storage, and code generation to make queries fast. At the same time, it scales to thousands of nodes and multi-hour queries using the Spark engine, which provides full mid-query fault tolerance. Spark SQL is a Spark module for structured data processing.  Spark SQL is used to execute SQL queries and also be used to read data from an existing Hive installation.\n",
        "\n",
        "* Amazon Redshift: is a fully managed, petabyte-scale data warehouse service in the cloud. We use Amazon Redshift to load the data and run queries on the data.\n",
        "We can also create additional databases as needed by running an SQL command. Most important we can scale it from a hundred gigabytes of data to a petabyte or more. The Amazon Redshift service manages all of the work of setting up, operating and scaling a data warehouse. These tasks include provisioning capacity, monitoring, and backing of the cluster, and applying patches and upgrades to the Amazon Redshift engine.\n",
        "\n",
        "* Presto: is an open source distributed SQL query engine for running interactive analytic queries against data sources of all sizes ranging from gigabytes to petabytes. Presto allows querying data where it lives, including Hive, Cassandra, relational databases or even proprietary data stores.\n",
        "A single Presto query can combine data from multiple sources, allowing for analytics across your entire organization. Presto is targeted at analysts who expect response times ranging from sub-second to minutes."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_GEtsSdTOJHu",
        "colab_type": "text"
      },
      "source": [
        "## Data Visualization Layer\n",
        "\n",
        "The visualization, or presentation tier, probably the most prestigious tier, where the data pipeline users may feel the VALUE of DATA. We need something that will grab people’s attention, pull them into, make your findings well-understood.\n",
        "\n",
        "### Tools\n",
        "\n",
        "* Tableau Server\n",
        "* Power Bi Server\n",
        "* Tibco Spotfire\n",
        "* Kibana"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H7Y7yEpbLW1U",
        "colab_type": "text"
      },
      "source": [
        "## Data Security Layer\n",
        "\n",
        "Security is the primary task of any work. Security should be implemented at all layers of the lake starting from Ingestion, through Storage, Analytics, Discovery, all the way to Consumption.\n",
        "\n",
        "Some responsibilities for the security layer are:\n",
        "- Big Data Authentication (Kerberos)\n",
        "- Access Control over the datasets.\n",
        "- Encryption and Data Masking\n",
        "- Auditing data access by users"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "osMfQ4wJM223",
        "colab_type": "text"
      },
      "source": [
        "## IoT Analytics Platform for Real-Time Data Ingestion, Streaming Analytics\n",
        "\n",
        "![Streaming](https://github.com/jcestevezc/Cloudera/blob/master/Architecture%20Intro/streaming.png?raw=true)\n",
        "\n",
        "[More info](https://www.xenonstack.com/blog/iot-analytics-platform/)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BYLz5y9eLVOw",
        "colab_type": "text"
      },
      "source": [
        "## Data Pipeline\n",
        "\n",
        "Data Pipeline the main component of Data Integration. All transformation of data happens in Data Pipeline.\n",
        "\n",
        "Activities in a data pipeline:\n",
        "\n",
        "* Data Ingestion\n",
        "* Data Integration\n",
        "* Data Organization\n",
        "* Data Refining\n",
        "* Data Analytics\n",
        "\n",
        "Some typical storage layers for a Data Pipeline can include:\n",
        "\n",
        "* Ingestion or raw layer\n",
        "* Cleaning or transforming layer\n",
        "* Merging or Integration\n",
        "* Modelling layer for analytical activities"
      ]
    }
  ]
}