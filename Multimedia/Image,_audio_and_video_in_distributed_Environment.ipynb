{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Image, audio and video in distributed Environment.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyNWj5ZTTiO6Yi2pCRmsmxpR",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jcestevezc/Cloudera/blob/master/Multimedia/Image%2C_audio_and_video_in_distributed_Environment.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c8xaOy5s6tss",
        "colab_type": "text"
      },
      "source": [
        "# Storage in a distributed environment"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2rXbQALl65i1",
        "colab_type": "text"
      },
      "source": [
        "## Option 1:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N245p9qx11pw",
        "colab_type": "text"
      },
      "source": [
        "Hadoop provides us the facility to read/write binary files. So, practically anything which can be converted into bytes can be stored into HDFS(images, videos etc). To do that Hadoop provides something called asSequenceFiles. SequenceFile is a flat file consisting of binary key/value pairs. \n",
        "\n",
        "The SequenceFile provides a Writer, Reader and Sorter classes for writing, reading and sorting respectively. So, is necessary to convert the image/video file into a SeuenceFile and store it into the HDFS.\n",
        "\n",
        "Here is small piece of code that will take an image file and convert it into a SequenceFile, where name of the file is the key and image content is the value :"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6Ntb4Q2n5aFY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "public class ImageToSeq {\n",
        "    public static void main(String args[]) throws Exception {\n",
        "\n",
        "        Configuration confHadoop = new Configuration();     \n",
        "        confHadoop.addResource(new Path(\"/hadoop/projects/hadoop-1.0.4/conf/core-site.xml\"));\n",
        "        confHadoop.addResource(new Path(\"/hadoop/projects/hadoop-1.0.4/conf/hdfs-site.xml\"));   \n",
        "        FileSystem fs = FileSystem.get(confHadoop);\n",
        "        Path inPath = new Path(\"/mapin/1.png\");\n",
        "        Path outPath = new Path(\"/mapin/11.png\");\n",
        "        FSDataInputStream in = null;\n",
        "        Text key = new Text();\n",
        "        BytesWritable value = new BytesWritable();\n",
        "        SequenceFile.Writer writer = null;\n",
        "        try{\n",
        "            in = fs.open(inPath);\n",
        "            byte buffer[] = new byte[in.available()];\n",
        "            in.read(buffer);\n",
        "            writer = SequenceFile.createWriter(fs, confHadoop, outPath, key.getClass(),value.getClass());\n",
        "            writer.append(new Text(inPath.getName()), new BytesWritable(buffer));\n",
        "        }catch (Exception e) {\n",
        "            System.out.println(\"Exception MESSAGES = \"+e.getMessage());\n",
        "        }\n",
        "        finally {\n",
        "            IOUtils.closeStream(writer);\n",
        "            System.out.println(\"last line of the code....!!!!!!!!!!\");\n",
        "        }\n",
        "    }\n",
        "}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X4zJkiw711yD",
        "colab_type": "text"
      },
      "source": [
        "## Option 2:\n",
        "\n",
        "If the intention is to just dump the files as it is, with this command is possible:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EtJSBM-b5yL1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "bin/hadoop fs -put /src_image_file /dst_image_file"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X79vDSQe6Uh1",
        "colab_type": "text"
      },
      "source": [
        "# **How to Analyze Video Data Using Hadoop?**\n",
        "\n",
        "![logo](https://static1.tothenew.com/blog/wp-content/uploads/2016/11/hipi.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RtWXr_Ka6KKe",
        "colab_type": "text"
      },
      "source": [
        "HIPI is a library for Hadoop's MapReduce framework that provides an API for performing image processing tasks in a distributed computing environment.\n",
        "\n",
        "## Operations Performed During Video Analytics using Hadoop\n",
        "\n",
        "![hipi](https://static1.tothenew.com/blog/wp-content/uploads/2016/11/blog-on-video-analytics-1-.png)\n",
        "\n",
        "1) Conversion of Video into Frames: JCodec is an open source library for video codecs and formats that is  implemented on Java.There are various tools for the digital transcoding of the video data into frames such as JCodec, Xuggler.\n",
        "\n",
        "The following code is used to convert the video into a frame:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1YFL0l8I_ahv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "int frameNo = 785;\n",
        "BufferedImage frame1 = FrameGrab.getFrame(new File(\"video.mp4\"), frameNo);\n",
        "ImageIO.write(frame1, \"png\", new File(\"abc.png\"));"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1ti3rwy-_ej3",
        "colab_type": "text"
      },
      "source": [
        "2) Put Frames in the HDFS: Putting frames or images in the HDFS using the put command is not possible. So to store the images or frames into the HDFS, first convert the frames as the stream of bytes and then store in HDFS. Hadoop provides us the facility to read/write binary files. So, practically anything which can be converted into bytes can be stored in HDFS.\n",
        "\n",
        "3) Store images in an HIPI ImageBundle: After the process of transcoding the images, these are combined into a single large file so that it can easily be managed and analyzed. Using the add image method, we can add every image into the HIPI imageBundle. So HIPI ImageBundle can be considered as a bunch of Images. Each mapper will generate an HIPI ImageBundle, and the Reducer will merge all  bundles into a single large bundle. By storing images in this way now you are able to work on HIPI framework. Now MapReduce jobs are running on these image Bundles for image analysis.\n",
        "\n",
        "4) Analysis Of Frame by HIPI Framework: HIPI is an image processing library designed to process a large number of images with the help of Hadoop MapReduce parallel programming framework. HIPI facilitates efficient and high-throughput image processing with MapReduce style parallel programs typically executed on a cluster. It provides a solution to store a large collection of images on the Hadoop Distributed File System (HDFS) and make them available for efficient distributed processing."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e564XuLU113c",
        "colab_type": "text"
      },
      "source": [
        "**References**\n",
        "\n",
        "* https://community.cloudera.com/t5/Support-Questions/how-hadoop-stores-unstructured-data-like-image-audio-and/td-p/188234\n",
        "* https://stackoverflow.com/questions/16546040/store-images-videos-into-hadoop-hdfs\n",
        "* https://www.tothenew.com/blog/how-to-manage-and-analyze-video-data-using-hadoop/\n",
        "* https://tanzu.vmware.com/content/blog/using-hadoop-mapreduce-for-distributed-video-transcoding"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dqMS1kwf111c",
        "colab_type": "text"
      },
      "source": [
        ""
      ]
    }
  ]
}