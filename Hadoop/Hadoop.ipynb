{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hadoop 2.6.0-cdh5.4.2\r\n",
      "Subversion http://github.com/cloudera/hadoop -r 15b703c8725733b7b2813d2325659eb7d57e7a3f\r\n",
      "Compiled by jenkins on 2015-05-20T00:03Z\r\n",
      "Compiled with protoc 2.5.0\r\n",
      "From source with checksum de74f1adb3744f8ee85d9a5b98f90d\r\n",
      "This command was run using /usr/jars/hadoop-common-2.6.0-cdh5.4.2.jar\r\n"
     ]
    }
   ],
   "source": [
    "! hadoop version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Configured Capacity: 58665738240 (54.64 GB)\n",
      "Present Capacity: 45943443456 (42.79 GB)\n",
      "DFS Remaining: 45548781568 (42.42 GB)\n",
      "DFS Used: 394661888 (376.38 MB)\n",
      "DFS Used%: 0.86%\n",
      "Under replicated blocks: 0\n",
      "Blocks with corrupt replicas: 0\n",
      "Missing blocks: 0\n",
      "Missing blocks (with replication factor 1): 383\n",
      "\n",
      "-------------------------------------------------\n",
      "Live datanodes (1):\n",
      "\n",
      "Name: 10.0.2.15:50010 (quickstart.cloudera)\n",
      "Hostname: quickstart.cloudera\n",
      "Decommission Status : Normal\n",
      "Configured Capacity: 58665738240 (54.64 GB)\n",
      "DFS Used: 394661888 (376.38 MB)\n",
      "Non DFS Used: 12722294784 (11.85 GB)\n",
      "DFS Remaining: 45548781568 (42.42 GB)\n",
      "DFS Used%: 0.67%\n",
      "DFS Remaining%: 77.64%\n",
      "Configured Cache Capacity: 0 (0 B)\n",
      "Cache Used: 0 (0 B)\n",
      "Cache Remaining: 0 (0 B)\n",
      "Cache Used%: 100.00%\n",
      "Cache Remaining%: 0.00%\n",
      "Xceivers: 2\n",
      "Last contact: Fri Jan 24 10:04:37 PST 2020\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!hdfs dfsadmin -report"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Alternativamente se podria por el navegador a travez de la siguiente ruta http://quickstart.cloudera:50070/dfshealth.html#tab-overview"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Interactuando con el HDFS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Creacion de carpetas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "! hdfs dfs -mkdir -p /user/hadoop"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Listar carpetas y archivos del hdfs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 7 items\n",
      "drwxr-xr-x   - admin    supergroup          0 2015-06-09 03:38 /user/admin\n",
      "drwxr-xr-x   - cloudera cloudera            0 2015-06-09 03:37 /user/cloudera\n",
      "drwxr-xr-x   - cloudera supergroup          0 2020-01-24 10:11 /user/hadoop\n",
      "drwxr-xr-x   - mapred   hadoop              0 2015-06-09 03:37 /user/history\n",
      "drwxrwxrwx   - hive     hive                0 2015-06-09 03:37 /user/hive\n",
      "drwxrwxrwx   - oozie    oozie               0 2015-06-09 03:39 /user/oozie\n",
      "drwxr-xr-x   - spark    spark               0 2015-06-09 03:38 /user/spark\n"
     ]
    }
   ],
   "source": [
    "! hdfs dfs -ls /user"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Copiando archivos del sistema (nodo) al hdfs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "! hdfs dfs -put Downloads/big-data-4/daily_weather.csv /user/hadoop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 1 items\r\n",
      "-rw-r--r--   1 cloudera supergroup     171621 2020-01-24 10:22 /user/hadoop/daily_weather.csv\r\n"
     ]
    }
   ],
   "source": [
    "! hdfs dfs -ls /user/hadoop"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. Visualizando el contenido del archivo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "! hdfs dfs -cat /user/hadoop/daily_weather.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5. Eliminando archivos del hdfs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20/01/24 10:32:18 INFO fs.TrashPolicyDefault: Namenode trash configuration: Deletion interval = 0 minutes, Emptier interval = 0 minutes.\n",
      "Deleted /user/hadoop/daily_weather.csv\n"
     ]
    }
   ],
   "source": [
    "! hdfs dfs -rm /user/hadoop/daily_weather.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Procesamiento con Hadoop"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ejecucion de proceso hadoop para generacion de datos de ejemplo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20/01/24 11:48:41 INFO client.RMProxy: Connecting to ResourceManager at /0.0.0.0:8032\n",
      "20/01/24 11:48:49 INFO terasort.TeraSort: Generating 1000 using 2\n",
      "20/01/24 11:48:50 INFO mapreduce.JobSubmitter: number of splits:2\n",
      "20/01/24 11:48:51 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_1579369419243_0001\n",
      "20/01/24 11:48:55 INFO impl.YarnClientImpl: Submitted application application_1579369419243_0001\n",
      "20/01/24 11:48:56 INFO mapreduce.Job: The url to track the job: http://quickstart.cloudera:8088/proxy/application_1579369419243_0001/\n",
      "20/01/24 11:48:56 INFO mapreduce.Job: Running job: job_1579369419243_0001\n",
      "20/01/24 11:49:30 INFO mapreduce.Job: Job job_1579369419243_0001 running in uber mode : false\n",
      "20/01/24 11:49:30 INFO mapreduce.Job:  map 0% reduce 0%\n",
      "20/01/24 11:49:52 INFO mapreduce.Job:  map 100% reduce 0%\n",
      "20/01/24 11:49:55 INFO mapreduce.Job: Job job_1579369419243_0001 completed successfully\n",
      "20/01/24 11:49:55 INFO mapreduce.Job: Counters: 31\n",
      "\tFile System Counters\n",
      "\t\tFILE: Number of bytes read=0\n",
      "\t\tFILE: Number of bytes written=220422\n",
      "\t\tFILE: Number of read operations=0\n",
      "\t\tFILE: Number of large read operations=0\n",
      "\t\tFILE: Number of write operations=0\n",
      "\t\tHDFS: Number of bytes read=164\n",
      "\t\tHDFS: Number of bytes written=100000\n",
      "\t\tHDFS: Number of read operations=8\n",
      "\t\tHDFS: Number of large read operations=0\n",
      "\t\tHDFS: Number of write operations=4\n",
      "\tJob Counters \n",
      "\t\tLaunched map tasks=2\n",
      "\t\tOther local map tasks=2\n",
      "\t\tTotal time spent by all maps in occupied slots (ms)=35768\n",
      "\t\tTotal time spent by all reduces in occupied slots (ms)=0\n",
      "\t\tTotal time spent by all map tasks (ms)=35768\n",
      "\t\tTotal vcore-seconds taken by all map tasks=35768\n",
      "\t\tTotal megabyte-seconds taken by all map tasks=36626432\n",
      "\tMap-Reduce Framework\n",
      "\t\tMap input records=1000\n",
      "\t\tMap output records=1000\n",
      "\t\tInput split bytes=164\n",
      "\t\tSpilled Records=0\n",
      "\t\tFailed Shuffles=0\n",
      "\t\tMerged Map outputs=0\n",
      "\t\tGC time elapsed (ms)=244\n",
      "\t\tCPU time spent (ms)=2040\n",
      "\t\tPhysical memory (bytes) snapshot=304685056\n",
      "\t\tVirtual memory (bytes) snapshot=3124379648\n",
      "\t\tTotal committed heap usage (bytes)=220200960\n",
      "\torg.apache.hadoop.examples.terasort.TeraGen$Counters\n",
      "\t\tCHECKSUM=2173251765740\n",
      "\tFile Input Format Counters \n",
      "\t\tBytes Read=0\n",
      "\tFile Output Format Counters \n",
      "\t\tBytes Written=100000\n"
     ]
    }
   ],
   "source": [
    "! hadoop jar /usr/lib/hadoop-mapreduce/hadoop-mapreduce-examples.jar teragen 1000 /user/hadoop/terainput"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ordenamiento de los datos generados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20/01/24 12:06:00 INFO terasort.TeraSort: starting\n",
      "20/01/24 12:06:02 INFO input.FileInputFormat: Total input paths to process : 2\n",
      "Spent 250ms computing base-splits.\n",
      "Spent 3ms computing TeraScheduler splits.\n",
      "Computing input splits took 255ms\n",
      "Sampling 2 splits of 2\n",
      "Making 1 from 1000 sampled records\n",
      "Computing parititions took 212ms\n",
      "Spent 472ms computing partitions.\n",
      "20/01/24 12:06:02 INFO client.RMProxy: Connecting to ResourceManager at /0.0.0.0:8032\n",
      "20/01/24 12:06:03 INFO mapreduce.JobSubmitter: number of splits:2\n",
      "20/01/24 12:06:03 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_1579369419243_0002\n",
      "20/01/24 12:06:04 INFO impl.YarnClientImpl: Submitted application application_1579369419243_0002\n",
      "20/01/24 12:06:04 INFO mapreduce.Job: The url to track the job: http://quickstart.cloudera:8088/proxy/application_1579369419243_0002/\n",
      "20/01/24 12:06:04 INFO mapreduce.Job: Running job: job_1579369419243_0002\n",
      "20/01/24 12:06:15 INFO mapreduce.Job: Job job_1579369419243_0002 running in uber mode : false\n",
      "20/01/24 12:06:15 INFO mapreduce.Job:  map 0% reduce 0%\n",
      "20/01/24 12:06:31 INFO mapreduce.Job:  map 100% reduce 0%\n",
      "20/01/24 12:06:42 INFO mapreduce.Job:  map 100% reduce 100%\n",
      "20/01/24 12:06:42 INFO mapreduce.Job: Job job_1579369419243_0002 completed successfully\n",
      "20/01/24 12:06:43 INFO mapreduce.Job: Counters: 49\n",
      "\tFile System Counters\n",
      "\t\tFILE: Number of bytes read=104006\n",
      "\t\tFILE: Number of bytes written=542925\n",
      "\t\tFILE: Number of read operations=0\n",
      "\t\tFILE: Number of large read operations=0\n",
      "\t\tFILE: Number of write operations=0\n",
      "\t\tHDFS: Number of bytes read=100262\n",
      "\t\tHDFS: Number of bytes written=100000\n",
      "\t\tHDFS: Number of read operations=9\n",
      "\t\tHDFS: Number of large read operations=0\n",
      "\t\tHDFS: Number of write operations=2\n",
      "\tJob Counters \n",
      "\t\tLaunched map tasks=2\n",
      "\t\tLaunched reduce tasks=1\n",
      "\t\tData-local map tasks=2\n",
      "\t\tTotal time spent by all maps in occupied slots (ms)=27989\n",
      "\t\tTotal time spent by all reduces in occupied slots (ms)=7715\n",
      "\t\tTotal time spent by all map tasks (ms)=27989\n",
      "\t\tTotal time spent by all reduce tasks (ms)=7715\n",
      "\t\tTotal vcore-seconds taken by all map tasks=27989\n",
      "\t\tTotal vcore-seconds taken by all reduce tasks=7715\n",
      "\t\tTotal megabyte-seconds taken by all map tasks=28660736\n",
      "\t\tTotal megabyte-seconds taken by all reduce tasks=7900160\n",
      "\tMap-Reduce Framework\n",
      "\t\tMap input records=1000\n",
      "\t\tMap output records=1000\n",
      "\t\tMap output bytes=102000\n",
      "\t\tMap output materialized bytes=104012\n",
      "\t\tInput split bytes=262\n",
      "\t\tCombine input records=0\n",
      "\t\tCombine output records=0\n",
      "\t\tReduce input groups=1000\n",
      "\t\tReduce shuffle bytes=104012\n",
      "\t\tReduce input records=1000\n",
      "\t\tReduce output records=1000\n",
      "\t\tSpilled Records=2000\n",
      "\t\tShuffled Maps =2\n",
      "\t\tFailed Shuffles=0\n",
      "\t\tMerged Map outputs=2\n",
      "\t\tGC time elapsed (ms)=239\n",
      "\t\tCPU time spent (ms)=3560\n",
      "\t\tPhysical memory (bytes) snapshot=704208896\n",
      "\t\tVirtual memory (bytes) snapshot=4670615552\n",
      "\t\tTotal committed heap usage (bytes)=536870912\n",
      "\tShuffle Errors\n",
      "\t\tBAD_ID=0\n",
      "\t\tCONNECTION=0\n",
      "\t\tIO_ERROR=0\n",
      "\t\tWRONG_LENGTH=0\n",
      "\t\tWRONG_MAP=0\n",
      "\t\tWRONG_REDUCE=0\n",
      "\tFile Input Format Counters \n",
      "\t\tBytes Read=100000\n",
      "\tFile Output Format Counters \n",
      "\t\tBytes Written=100000\n",
      "20/01/24 12:06:43 INFO terasort.TeraSort: done\n"
     ]
    }
   ],
   "source": [
    "! hadoop jar /usr/lib/hadoop-mapreduce/hadoop-mapreduce-examples.jar terasort /user/hadoop/terainput /user/hadoop/teraoutput"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 3 items\r\n",
      "-rw-r--r--   1 cloudera supergroup          0 2020-01-24 12:06 /user/hadoop/teraoutput/_SUCCESS\r\n",
      "-rw-r--r--  10 cloudera supergroup          0 2020-01-24 12:06 /user/hadoop/teraoutput/_partition.lst\r\n",
      "-rw-r--r--   1 cloudera supergroup     100000 2020-01-24 12:06 /user/hadoop/teraoutput/part-r-00000\r\n"
     ]
    }
   ],
   "source": [
    "! hdfs dfs -ls /user/hadoop/teraoutput"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Borrando carpetas dentro del hdfs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deleted /user/hadoop/terainput\r\n",
      "Deleted /user/hadoop/teraoutput\r\n"
     ]
    }
   ],
   "source": [
    "! hdfs dfs -rm -r -skipTrash /user/hadoop/tera*"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
